import AVFoundation
import Speech
import os
import CoreMedia

/// SpeechAnalyzer å®ç°
/// é€‚ç”¨äº macOS 26+ / iOS 26+
/// æ–°ä¸€ä»£è®¾å¤‡ç«¯è¯­éŸ³è¯†åˆ«ï¼Œæ›´å¿«æ›´å‡†ç¡®
@available(macOS 26.0, iOS 26.0, *)
@MainActor
final class SpeechAnalyzerProvider: TranscriptionProvider {
    
    // MARK: - Properties
    
    let identifier = "speech_analyzer"
    let displayName = "Apple è¯­éŸ³åˆ†æå™¨"
    
    let capabilities: TranscriptionCapability = [.realtime, .offline, .longForm, .punctuation, .multilingual]
    
    var locale: Locale {
        didSet {
            // éœ€è¦é‡æ–°åˆ›å»º transcriber
            needsRecreate = true
        }
    }
    
    var isAvailable: Bool {
        SpeechTranscriber.isAvailable
    }
    
    var supportedLocales: [Locale] {
        // è¿”å›å¸¸è§æ”¯æŒçš„è¯­è¨€åˆ—è¡¨ï¼ˆé¿å… async è°ƒç”¨ï¼‰
        // å®é™…æ”¯æŒæƒ…å†µä¼šåœ¨ prepare() æ—¶æ£€æŸ¥
        [
            Locale(identifier: "zh-Hans"),
            Locale(identifier: "zh-Hant"),
            Locale(identifier: "en-US"),
            Locale(identifier: "ja-JP"),
            Locale(identifier: "ko-KR"),
            Locale(identifier: "de-DE"),
            Locale(identifier: "fr-FR"),
            Locale(identifier: "es-ES"),
        ]
    }
    
    var onResult: ((TranscriptionResult) -> Void)?
    var onError: ((Error) -> Void)?
    
    // MARK: - Private
    
    private let logger = Logger(subsystem: "com.spokeanywhere", category: "SpeechAnalyzerProvider")
    
    private var analyzer: SpeechAnalyzer?
    private var transcriber: SpeechTranscriber?
    private var inputContinuation: AsyncStream<AnalyzerInput>.Continuation?
    private var resultsTask: Task<Void, Never>?
    private var analyzeTask: Task<Void, Never>?
    
    private var needsRecreate = false
    private var targetAudioFormat: AVAudioFormat?
    private var audioConverter: AVAudioConverter?
    
    // ç´¯ç§¯çš„æ–‡æœ¬
    private var finalizedText: String = ""   // å·²ç¡®è®¤çš„æ–‡æœ¬
    private var volatileText: String = ""    // å½“å‰é¢„è§ˆæ–‡æœ¬
    
    // MARK: - Init
    
    init(locale: Locale = Locale(identifier: "zh-Hans")) {
        self.locale = locale
    }
    
    // MARK: - TranscriptionProvider
    
    func requestAuthorization() async -> Bool {
        // SpeechAnalyzer ä½¿ç”¨è®¾å¤‡ç«¯å¤„ç†ï¼Œä¸»è¦éœ€è¦éº¦å…‹é£æƒé™
        let speechStatus = await withCheckedContinuation { continuation in
            SFSpeechRecognizer.requestAuthorization { status in
                continuation.resume(returning: status == .authorized)
            }
        }
        return speechStatus
    }
    
    func prepare() async throws {
        try await setupSpeechAnalyzer()
        logger.info("âœ… SpeechAnalyzerProvider prepared")
    }
    
    func process(buffer: AVAudioPCMBuffer) throws {
        guard let continuation = inputContinuation else {
            throw TranscriptionError.engineNotReady
        }
        
        // è½¬æ¢éŸ³é¢‘æ ¼å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
        let convertedBuffer: AVAudioPCMBuffer
        if let targetFormat = targetAudioFormat, buffer.format != targetFormat {
            convertedBuffer = try convertBuffer(buffer, to: targetFormat)
        } else {
            convertedBuffer = buffer
        }
        
        // åˆ›å»º AnalyzerInput å¹¶å‘é€
        let input = AnalyzerInput(buffer: convertedBuffer)
        continuation.yield(input)
    }
    
    /// è½¬æ¢éŸ³é¢‘ç¼“å†²åŒºæ ¼å¼
    private func convertBuffer(_ buffer: AVAudioPCMBuffer, to format: AVAudioFormat) throws -> AVAudioPCMBuffer {
        // åˆ›å»ºæˆ–å¤ç”¨è½¬æ¢å™¨
        if audioConverter == nil || audioConverter?.inputFormat != buffer.format || audioConverter?.outputFormat != format {
            guard let converter = AVAudioConverter(from: buffer.format, to: format) else {
                throw TranscriptionError.processingFailed("Cannot create audio converter")
            }
            audioConverter = converter
        }
        
        guard let converter = audioConverter else {
            throw TranscriptionError.processingFailed("Audio converter not available")
        }
        
        // è®¡ç®—è¾“å‡ºå¸§æ•°
        let ratio = format.sampleRate / buffer.format.sampleRate
        let outputFrameCount = AVAudioFrameCount(Double(buffer.frameLength) * ratio)
        
        guard let outputBuffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: outputFrameCount) else {
            throw TranscriptionError.processingFailed("Cannot create output buffer")
        }
        
        var error: NSError?
        let inputBlock: AVAudioConverterInputBlock = { _, outStatus in
            outStatus.pointee = .haveData
            return buffer
        }
        
        converter.convert(to: outputBuffer, error: &error, withInputFrom: inputBlock)
        
        if let error = error {
            throw TranscriptionError.processingFailed("Conversion failed: \(error.localizedDescription)")
        }
        
        return outputBuffer
    }
    
    func finishProcessing() async throws {
        // ç»“æŸè¾“å…¥æµ
        inputContinuation?.finish()
        
        // ç­‰å¾…åˆ†æå®Œæˆ
        if let analyzer = analyzer {
            try await analyzer.finalizeAndFinishThroughEndOfInput()
        }
        
        // ç­‰å¾…ç»“æœå¤„ç†å®Œæˆ
        await resultsTask?.value
        
        logger.info("âœ… SpeechAnalyzerProvider finished processing")
    }
    
    func cancel() {
        inputContinuation?.finish()
        resultsTask?.cancel()
        analyzeTask?.cancel()
        
        Task {
            await analyzer?.cancelAndFinishNow()
        }
        
        cleanup()
        logger.info("ğŸš« SpeechAnalyzerProvider cancelled")
    }
    
    func reset() {
        cleanup()
        finalizedText = ""
        volatileText = ""
        needsRecreate = true
        logger.info("ğŸ”„ SpeechAnalyzerProvider reset")
    }
    
    // MARK: - Private - SpeechAnalyzer Setup
    
    private func setupSpeechAnalyzer() async throws {
        // Step 1: è·å–æ”¯æŒçš„ locale
        guard let supportedLocale = await SpeechTranscriber.supportedLocale(equivalentTo: locale) else {
            logger.error("âŒ Locale not supported: \(self.locale.identifier)")
            throw TranscriptionError.unsupportedLocale(locale)
        }
        
        // Step 2: åˆ›å»º SpeechTranscriber
        // ä½¿ç”¨ progressiveTranscription é¢„è®¾æ”¯æŒå®æ—¶è½¬å½•
        let transcriber = SpeechTranscriber(
            locale: supportedLocale,
            preset: .progressiveTranscription
        )
        self.transcriber = transcriber
        
        // Step 3: æ£€æŸ¥å¹¶å®‰è£…æ‰€éœ€èµ„äº§
        try await ensureAssetsInstalled(for: transcriber)
        
        // Step 4: è·å–æœ€ä½³éŸ³é¢‘æ ¼å¼
        guard let format = await SpeechAnalyzer.bestAvailableAudioFormat(compatibleWith: [transcriber]) else {
            logger.error("âŒ No compatible audio format available")
            throw TranscriptionError.processingFailed("No compatible audio format")
        }
        self.targetAudioFormat = format
        logger.info("ğŸ“¢ Using audio format: \(format)")
        
        // Step 5: åˆ›å»ºè¾“å…¥æµ
        let (inputSequence, inputBuilder) = AsyncStream.makeStream(of: AnalyzerInput.self)
        self.inputContinuation = inputBuilder
        
        // Step 6: åˆ›å»º SpeechAnalyzer
        let analyzer = SpeechAnalyzer(modules: [transcriber])
        self.analyzer = analyzer
        
        // Step 7: é¢„çƒ­åˆ†æå™¨
        try await analyzer.prepareToAnalyze(in: format)
        
        // Step 8: å¯åŠ¨ç»“æœç›‘å¬
        resultsTask = Task { [weak self] in
            await self?.listenForResults(transcriber: transcriber)
        }
        
        // Step 9: å¯åŠ¨åˆ†æï¼ˆè‡ªä¸»æ¨¡å¼ï¼‰
        analyzeTask = Task { [weak self] in
            do {
                try await analyzer.start(inputSequence: inputSequence)
            } catch {
                await MainActor.run {
                    self?.onError?(error)
                }
            }
        }
        
        logger.info("âœ… SpeechAnalyzer setup complete for locale: \(supportedLocale.identifier)")
    }
    
    private func ensureAssetsInstalled(for transcriber: SpeechTranscriber) async throws {
        // æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½èµ„äº§
        if let installationRequest = try await AssetInventory.assetInstallationRequest(supporting: [transcriber]) {
            logger.info("ğŸ“¥ Downloading speech assets...")
            
            // ä¸‹è½½å¹¶å®‰è£…
            try await installationRequest.downloadAndInstall()
            
            logger.info("âœ… Speech assets installed")
        } else {
            logger.info("âœ… Speech assets already installed")
        }
    }
    
    private func listenForResults(transcriber: SpeechTranscriber) async {
        do {
            for try await result in transcriber.results {
                await handleResult(result)
            }
        } catch {
            await MainActor.run { [weak self] in
                self?.logger.error("âŒ Results error: \(error.localizedDescription)")
                self?.onError?(error)
            }
        }
    }
    
    private func handleResult(_ result: SpeechTranscriber.Result) async {
        // ä» AttributedString æå–çº¯æ–‡æœ¬
        let segmentText = String(result.text.characters)
        let isFinal = result.isFinal
        
        await MainActor.run { [weak self] in
            guard let self = self else { return }
            
            if isFinal {
                // Final ç»“æœï¼šç´¯ç§¯åˆ° finalizedText
                self.finalizedText += segmentText
                self.volatileText = ""  // æ¸…ç©º volatile
                
                self.logger.info("ğŸ“ Finalized segment: \(segmentText)")
                self.logger.info("ğŸ“ Total: \(self.finalizedText)")
            } else {
                // Volatile ç»“æœï¼šæ›´æ–°é¢„è§ˆ
                self.volatileText = segmentText
                
                self.logger.debug("ğŸ“ Volatile: \(segmentText)")
            }
            
            // ä½¿ç”¨æ–°çš„æ„é€ å™¨ï¼Œåˆ†ç¦» finalized å’Œ volatile
            let transcriptionResult = TranscriptionResult(
                finalizedText: self.finalizedText,
                volatileText: self.volatileText,
                type: .partial  // å½•éŸ³æœªç»“æŸï¼Œå§‹ç»ˆæ˜¯ partial
            )
            self.onResult?(transcriptionResult)
        }
    }
    
    private func cleanup() {
        inputContinuation = nil
        resultsTask = nil
        analyzeTask = nil
        analyzer = nil
        transcriber = nil
        targetAudioFormat = nil
        audioConverter = nil
    }
}

// MARK: - Provider Info

@available(macOS 26.0, iOS 26.0, *)
extension SpeechAnalyzerProvider {
    static var info: TranscriptionProviderInfo {
        TranscriptionProviderInfo(
            identifier: "speech_analyzer",
            displayName: "Apple è¯­éŸ³åˆ†æå™¨",
            description: "æ–°ä¸€ä»£è®¾å¤‡ç«¯è¯­éŸ³è¯†åˆ«ï¼Œæ›´å¿«æ›´å‡†ç¡®ï¼Œå®Œå…¨ç¦»çº¿ï¼Œé€‚ç”¨äº macOS 26+",
            capabilities: [.realtime, .offline, .longForm, .punctuation, .multilingual],
            minOSVersion: "macOS 26.0",
            isAvailable: SpeechTranscriber.isAvailable
        )
    }
}

// MARK: - Availability Check

enum SpeechAnalyzerAvailability {
    /// æ£€æŸ¥å½“å‰ç³»ç»Ÿæ˜¯å¦æ”¯æŒ SpeechAnalyzer
    static var isSupported: Bool {
        if #available(macOS 26.0, iOS 26.0, *) {
            return true
        }
        return false
    }
}
